Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Bahmani2011,
abstract = {1 Pig. http://hadoop.apache.org/pig. 2 K. Avrachenkov, N. Litvak, D. Nemirovsky, and N. Osipova. Monte carlo methods in pagerank computation: When one iteration is sufficient. SIAM J. Numer. Anal., 45(2):890--904, 2007.},
author = {Bahmani, Bahman and Chakrabarti, Kaushik and Xin, Dong},
booktitle = {Proc. ACM SIGMOD International Conference on Management of Data},
doi = {10.1145/1989323.1989425},
file = {:E$\backslash$:/论文集/Fast Personalized PageRank on MapReduce.pdf:pdf},
isbn = {9781450306614},
issn = {07308078},
keywords = {MapReduce},
pages = {973--984},
title = {{Fast personalized PageRank on MapReduce}},
url = {http://doi.acm.org/10.1145/1989323.1989425{\%}5Cnpapers2://publication/uuid/0688DA04-D1B2-49EB-90D2-DAAD64705189},
year = {2011}
}
@article{Corbett2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Corbett, James C and Dean, Jeffrey and Epstein, Michael and Fikes, Andrew and Frost, Christopher and Furman, J J and Ghemawat, Sanjay and Gubarev, Andrey and Heiser, Christopher and Hochschild, Peter and Hsieh, Wilson and Kanthak, Sebastian and Kogan, Eugene and Li, Hongyi and Lloyd, Alexander and Melnik, Sergey and Mwaura, David and Nagle, David and Quinlan, Sean and Rao, Rajesh and Rolig, Lindsay and Saito, Yasushi and Szymaniak, Michal and Taylor, Christopher and Wang, Ruth and Woodford, Dale},
doi = {10.1145/2491245},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Corbett et al. - 2012 - Spanner Google ' s Globally-Distributed Database.pdf:pdf},
isbn = {978-931971-96-6},
issn = {07342071},
journal = {Osdi},
pages = {251--264},
pmid = {191},
title = {{Spanner : Google ' s Globally-Distributed Database}},
year = {2012}
}
@article{Joulin2016,
abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
archivePrefix = {arXiv},
arxivId = {1607.01759},
author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
doi = {1511.09249v1},
eprint = {1607.01759},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
pmid = {1000303116},
title = {{Bag of Tricks for Efficient Text Classification}},
url = {http://arxiv.org/abs/1607.01759},
year = {2016}
}
@article{Bordes2014,
abstract = {Recent years have witnessed a proliferation of large-scale knowledge graphs, such as Freebase, YAGO, Google's Knowledge Graph, and Microsoft's Satori. Whereas there is a large body of research on mining homogeneous graphs, this new generation of information networks are highly heterogeneous, with thousands of entity and relation types and billions of instances of vertices and edges. In this tutorial, we will present the state of the art in constructing, mining, and growing knowledge graphs. The purpose of the tutorial is to equip newcomers to this exciting field with an understanding of the basic concepts, tools and methodologies, available datasets, and open research challenges. A publicly available knowledge base (Freebase) will be used throughout the tutorial to exemplify the different techniques.},
author = {Bordes, Antoine and Gabrilovich, Evgeniy},
doi = {10.1145/2623330.2630803},
file = {:E$\backslash$:/论文集/KDD14-T2-Bordes-Gabrilovich-KnowledgeGraph-Overview.pdf:pdf},
isbn = {9781450329569},
journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
pages = {1967--1967},
title = {{Constructing and mining web-scale knowledge graphs}},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2630803},
year = {2014}
}
@article{Bizer2009,
author = {Bizer, Christian and Lehmann, Jens and Kobilarov, Georgi and Auer, Soren and Becker, Christian and Cyganiak, Richard and Hellamnn, Sebastian},
file = {:E$\backslash$:/知识图谱/DBpedia  A crystallization point for the Web of Data - 副本.pdf:pdf},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
keywords = {knowledge extraction,linked data,rdf,web of data,wikipedia},
pages = {154--165},
title = {{DBpedia - A cystallization point for the Web of Data}},
volume = {7},
year = {2009}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Dean2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
editor = {Daniel, L Purich},
eprint = {10.1.1.163.5292},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dean, Ghemawat - 2008 - MapReduce Simplified Data Processing on Large Clusters.pdf:pdf},
institution = {Google, Inc.},
isbn = {9781595936868},
issn = {00010782},
journal = {Communications of the ACM},
number = {1},
pages = {1--13},
pmid = {11687618},
publisher = {ACM},
series = {SIGMOD '07},
title = {{MapReduce : Simplified Data Processing on Large Clusters}},
url = {http://portal.acm.org/citation.cfm?id=1327492},
volume = {51},
year = {2008}
}
@article{Zaharia2012,
abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
archivePrefix = {arXiv},
arxivId = {EECS-2011-82},
author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur},
doi = {10.1111/j.1095-8649.2005.00662.x},
eprint = {EECS-2011-82},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaharia et al. - 2012 - Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing.pdf:pdf},
isbn = {978-931971-92-8},
issn = {00221112},
journal = {NSDI'12 Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation},
pages = {2--2},
pmid = {2011},
title = {{Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing}},
url = {https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf},
year = {2012}
}
@article{Galarraga2015,
author = {Gal{\'{a}}rraga, Luis and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian M.},
doi = {10.1007/s00778-015-0394-1},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal{\'{a}}rraga et al. - 2015 - Fast rule mining in ontological knowledge bases with AMIE.pdf:pdf},
isbn = {1066-8888},
issn = {0949877X},
journal = {VLDB Journal},
keywords = {ILP,Inductive logic programming,Knowledge bases,Rule mining},
number = {6},
pages = {707--730},
title = {{Fast rule mining in ontological knowledge bases with AMIE+}},
volume = {24},
year = {2015}
}
@article{Lofgren2014,
abstract = {We propose a new algorithm, FAST-PPR, for estimating personalized PageRank: given start node {\$}s{\$} and target node {\$}t{\$} in a directed graph, and given a threshold {\$}\backslashdelta{\$}, FAST-PPR estimates the Personalized PageRank {\$}\backslashpi{\_}s(t){\$} from {\$}s{\$} to {\$}t{\$}, guaranteeing a small relative error as long {\$}\backslashpi{\_}s(t){\textgreater}\backslashdelta{\$}. Existing algorithms for this problem have a running-time of {\$}\backslashOmega(1/\backslashdelta){\$}; in comparison, FAST-PPR has a provable average running-time guarantee of {\$}{\{}O{\}}(\backslashsqrt{\{}d/\backslashdelta{\}}){\$} (where {\$}d{\$} is the average in-degree of the graph). This is a significant improvement, since {\$}\backslashdelta{\$} is often {\$}O(1/n){\$} (where {\$}n{\$} is the number of nodes) for applications. We also complement the algorithm with an {\$}\backslashOmega(1/\backslashsqrt{\{}\backslashdelta{\}}){\$} lower bound for PageRank estimation, showing that the dependence on {\$}\backslashdelta{\$} cannot be improved. We perform a detailed empirical study on numerous massive graphs, showing that FAST-PPR dramatically outperforms existing algorithms. For example, on the 2010 Twitter graph with 1.5 billion edges, for target nodes sampled by popularity, FAST-PPR has a {\$}20{\$} factor speedup over the state of the art. Furthermore, an enhanced version of FAST-PPR has a {\$}160{\$} factor speedup on the Twitter graph, and is at least {\$}20{\$} times faster on all our candidate graphs.},
archivePrefix = {arXiv},
arxivId = {1404.3181},
author = {Lofgren, Peter and Banerjee, Siddhartha and Goel, Ashish and Seshadhri, C.},
eprint = {1404.3181},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lofgren et al. - 2014 - FAST-PPR Scaling Personalized PageRank Estimation for Large Graphs.pdf:pdf},
isbn = {9781450329569},
keywords = {personalized pagerank,social search},
title = {{FAST-PPR: Scaling Personalized PageRank Estimation for Large Graphs}},
url = {http://arxiv.org/abs/1404.3181},
year = {2014}
}
@article{Cohen2016,
abstract = {Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into "deep" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are "unrolled" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1605.06523},
author = {Cohen, William W.},
eprint = {1605.06523},
file = {:E$\backslash$:/知识图谱/TensorLog A Differentiable Deductive Database.pdf:pdf},
number = {Nips},
title = {{TensorLog: A Differentiable Deductive Database}},
url = {http://arxiv.org/abs/1605.06523},
year = {2016}
}
@article{Wang2015,
abstract = {One important challenge for probabilistic logics is reasoning with very large knowledge bases (KBs) of imperfect information, such as those produced by modern web-scale information extraction systems. One scalability problem shared by many probabilistic logics is that answering queries involves "grounding" the query---i.e., mapping it to a propositional representation---and the size of a "grounding" grows with database size. To address this bottleneck, we present a first-order probabilistic language called ProPPR in which that approximate "local groundings" can be constructed in time independent of database size. Technically, ProPPR is an extension to stochastic logic programs (SLPs) that is biased towards short derivations; it is also closely related to an earlier relational learning algorithm called the path ranking algorithm (PRA). We show that the problem of constructing proofs for this logic is related to computation of personalized PageRank (PPR) on a linearized version of the proof space, and using on this connection, we develop a proveably-correct approximate grounding scheme, based on the PageRank-Nibble algorithm. Building on this, we develop a fast and easily-parallelized weight-learning algorithm for ProPPR. In experiments, we show that learning for ProPPR is orders magnitude faster than learning for Markov logic networks; that allowing mutual recursion (joint learning) in KB inference leads to improvements in performance; and that ProPPR can learn weights for a mutually recursive program with hundreds of clauses, which define scores of interrelated predicates, over a KB containing one million entities.},
archivePrefix = {arXiv},
arxivId = {1404.3301},
author = {Wang, William Yang and Mazaitis, Kathryn and Lao, Ni and Cohen, William W.},
doi = {10.1007/s10994-015-5488-x},
eprint = {1404.3301},
file = {:E$\backslash$:/知识图谱/ProPPR Efficient First-Order Probabilistic Logic Programming for structure discovery，parameter learning and scalable inference.pdf:pdf},
isbn = {1404.3301},
issn = {15730565},
journal = {Machine Learning},
keywords = {Personalized PageRank,Probabilistic logic,Scalable learning},
number = {1},
pages = {101--126},
title = {{Efficient inference and learning in a large knowledge base: Reasoning with extracted information using a locally groundable first-order probabilistic logic}},
volume = {100},
year = {2015}
}
@article{Malewicz2010,
abstract = {Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.},
author = {Malewicz, Grzegorz and Austern, Matthew H. and Bik, Aart J.C and Dehnert, James C. and Horn, Ilan and Leiser, Naty and Czajkowski, Grzegorz},
doi = {10.1145/1807167.1807184},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malewicz et al. - 2010 - Pregel a system for large-scale graph processing.pdf:pdf},
isbn = {9781450300322},
issn = {1450300324},
journal = {Proceedings of the 2010 international conference on Management of data - SIGMOD '10},
keywords = {distributed computing,graph algorigthms},
pages = {135--146},
title = {{Pregel: a system for large-scale graph processing}},
url = {http://dl.acm.org/citation.cfm?id=1807167.1807184},
year = {2010}
}
@article{Isard2007,
abstract = {Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad applica- tion combines computational “vertices” with communica- tion “channels” to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through files, TCP pipes, and shared-memory FIFOs. The vertices provided by the application developer are quite simple and are usually written as sequential programs with no thread creation or locking. Concurrency arises from Dryad scheduling vertices to run simultaneously on multi- ple computers, or on multiple CPU cores within a computer. The application can discover the size and placement of data at run time, and modify the graph as the computation pro- gresses to make efficient use of the available resources. Dryad is designed to scale from powerful multi-core sin- gle computers, through small clusters of computers, to data centers with thousands of computers. The Dryad execution engine handles all the difficult problems of creating a large distributed, concurrent application: scheduling the use of computers and their CPUs, recovering from communication or computer failures, and transporting data between ver- tices.},
author = {Isard, Michael and Budiu, Mihai and Yu, Yuan and Birrell, Andrew and Fetterly, Dennis},
doi = {10.1145/1272998.1273005},
file = {:E$\backslash$:/论文集/dryad.pdf:pdf},
isbn = {978-1-59593-636-3},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
keywords = {Distributed algorithm,cluster,concurrency,dataflow,distributed programming},
pages = {59--72},
title = {{Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks}},
year = {2007}
}
@inproceedings{Wang2016,
author = {Wang, Quan and Liu, Jing and Luo, Yuanfei and Wang, Bin and Lin, Chin-yew},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
file = {:E$\backslash$:/论文集/acl2016{\_}camera-ready.pdf:pdf},
isbn = {9781510827585},
pages = {1308--1318},
title = {{Knowledge base completion via coupled path ranking}},
year = {2016}
}
@article{Wang2014,
abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
file = {:E$\backslash$:/知识图谱/Knowledge Graph Embedding by Translating on Hyperplanes.pdf:pdf},
isbn = {9781577356783},
journal = {AAAI Conference on Artificial Intelligence},
keywords = {Knowledge Embedding,Knowledge Graph,TransH},
pages = {1112--1119},
title = {{Knowledge Graph Embedding by Translating on Hyperplanes}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=3BD702B75032E3D2814951AE36786825?doi=10.1.1.486.2800{\&}rep=rep1{\&}type=pdf},
year = {2014}
}
@article{Quinlan1993,
abstract = {FOIL is a learning system that constructs Horn clause programs from examples. This paper summarises the development of FOIL from 1989 up to early 1993 and evaluates its effectiveness on a non-trivial sequence of learning tasks taken from a Prolog programming text. Although many of these tasks are handled reasonably well, the experiment highlights some weaknesses of the current implementation. Areas for further research are identified.},
author = {Quinlan, J Ross and Cameron-Jones, R M},
doi = {10.1007/3-540-56602-3},
file = {:E$\backslash$:/知识图谱/FOIL A Midterm Report.pdf:pdf},
isbn = {978-3-540-56602-1},
journal = {Ecml},
pages = {3},
title = {{FOIL: A midterm report}},
year = {1993}
}
@article{Li,
author = {Li, Mu and Andersen, David G and Smola, Alexander and Yu, Kai},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - Unknown - Communication Efficient Distributed Machine Learning with the Parameter Server.pdf:pdf},
isbn = {9781931971164},
issn = {10495258},
pages = {1--9},
title = {{Communication Efficient Distributed Machine Learning with the Parameter Server}}
}
@article{Chen2017,
abstract = {This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
archivePrefix = {arXiv},
arxivId = {1704.00051},
author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
eprint = {1704.00051},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2017 - Reading Wikipedia to Answer Open-Domain Questions.pdf:pdf},
title = {{Reading Wikipedia to Answer Open-Domain Questions}},
url = {http://arxiv.org/abs/1704.00051},
year = {2017}
}
@article{Neelakantan2015,
abstract = {Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z) -{\textgreater} containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recursive neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11{\%}, and a method leveraging pre-trained embeddings by 7{\%}.},
archivePrefix = {arXiv},
arxivId = {1504.06662},
author = {Neelakantan, Arvind and Roth, Benjamin and McCallum, Andrew},
doi = {10.3115/v1/P15-1016},
eprint = {1504.06662},
file = {:E$\backslash$:/论文集/Compositional Vector Space Models for Knowledge Base Completion.pdf:pdf},
isbn = {9781941643723},
journal = {AAAI Spring Symposium Series},
keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
pages = {156--166},
title = {{Compositional Vector Space Models for Knowledge Base Completion}},
url = {http://aclweb.org/anthology/P15-1016},
year = {2015}
}
@article{Zhou2017,
abstract = {Abstract Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in cQA archives aims to find the existing questions that are  ...},
author = {Zhou, Guangyou and Huang, Jimmy},
doi = {10.1109/TKDE.2017.2665625},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Huang - 2017 - Modeling and Learning Continuous Word Embedding with Metadata for Question Retrieval.pdf:pdf},
isbn = {9781941643723},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
pages = {1--1},
title = {{Modeling and Learning Continuous Word Embedding with Metadata for Question Retrieval}},
url = {http://ieeexplore.ieee.org/document/7847319/},
year = {2017}
}
@article{Xin2013,
abstract = {From social networks to targeted advertising, big graphs capture the structure in data and are central to recent advances in machine learning and data mining. Unfortunately, directly applying existing data-parallel tools to graph computation tasks can be cumbersome and inefficient. The need for intuitive, scalable tools for graph computation has lead to the development of new graph-parallel systems (e.g. Pregel, PowerGraph) which are designed to efficiently execute graph algorithms. Unfortunately, these new graph-parallel systems do not address the challenges of graph construction and transformation which are often just as problematic as the subsequent computation. Furthermore, existing graph-parallel systems provide limited fault-tolerance and support for interactive data mining. We introduce GraphX, which combines the advantages of both data-parallel and graph-parallel systems by efficiently expressing graph computation within the Spark data-parallel framework. We leverage new ideas in distributed graph representation to efficiently distribute graphs as tabular data-structures. Similarly, we leverage advances in data-flow systems to exploit in-memory computation and fault-tolerance. We provide powerful new operations to simplify graph construction and transformation. Using these primitives we implement the PowerGraph and Pregel abstractions in less than 20 lines of code. Finally, by exploiting the Scala foundation of Spark, we enable users to interactively load, transform, and compute on massive graphs.},
archivePrefix = {arXiv},
arxivId = {1402.2394},
author = {Xin, Reynold S and Gonzalez, Joseph E and Franklin, Michael J and Stoica, Ion and AMPLab, Eecs},
doi = {10.1145/2484425.2484427},
eprint = {1402.2394},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xin et al. - 2013 - GraphX A Resilient Distributed Graph System on Spark.pdf:pdf},
isbn = {9781450321884},
issn = {0002-9513},
journal = {First International Workshop on Graph Data Management Experiences and Systems},
pages = {2},
title = {{GraphX: A Resilient Distributed Graph System on Spark}},
year = {2013}
}
@article{He2014,
abstract = {Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central tomost on- line advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3{\%}, an improvement with sig- nificant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surpris- ingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic re- gression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with. 1.},
author = {He, Xinran and Bowers, Stuart and Candela, Joaquin Qui{\~{n}}onero and Pan, Junfeng and Jin, Ou and Xu, Tianbing and Liu, Bo and Xu, Tao and Shi, Yanxin and Atallah, Antoine and Herbrich, Ralf},
doi = {10.1145/2648584.2648589},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2014 - Practical Lessons from Predicting Clicks on Ads at Facebook.pdf:pdf},
isbn = {9781450329996},
journal = {Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining - ADKDD'14},
pages = {1--9},
title = {{Practical Lessons from Predicting Clicks on Ads at Facebook}},
url = {http://dl.acm.org/citation.cfm?doid=2648584.2648589},
year = {2014}
}
@article{West2014,
author = {West, Robert and Gabrilovich, Evgeniy and Murphy, Kevin and Sun, Shaohua and Gupta, Rahul and Lin, Dekang},
file = {:E$\backslash$:/论文集/Knowledge Base Completion via Search-Based.pdf:pdf},
isbn = {9781450327442},
title = {{Knowledge Base Completion via Search-Based Question Answering}},
year = {2014}
}
@article{Nickel2011,
abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickel, Tresp, Kriegel - 2011 - A Three-Way Model for Collective Learning on Multi-Relational Data.pdf:pdf;:E$\backslash$:/知识图谱/A Three-Way Model for Collective Learning on Multi-Relational Data.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {28th International Conference on Machine Learning},
pages = {809----816},
title = {{A Three-Way Model for Collective Learning on Multi-Relational Data}},
year = {2011}
}
@misc{Ghemawat2003,
abstract = {We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.},
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
booktitle = {ACM SIGOPS Operating Systems Review},
doi = {10.1145/1165389.945450},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghemawat, Gobioff, Leung - 2003 - The Google file system.pdf:pdf},
isbn = {1581137575},
issn = {01635980},
number = {5},
pages = {29},
pmid = {191},
title = {{The Google file system}},
volume = {37},
year = {2003}
}
@article{Chen2016,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end- to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan- tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compres- sion and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
archivePrefix = {arXiv},
arxivId = {1603.02754},
author = {Chen, Tianqi and Guestrin, Carlos},
doi = {10.1145/2939672.2939785},
eprint = {1603.02754},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Guestrin - 2016 - XGBoost Reliable Large-scale Tree Boosting System(3).pdf:pdf},
isbn = {9781450342322},
journal = {arXiv},
keywords = {large-scale machine learning},
pages = {1--6},
title = {{XGBoost : Reliable Large-scale Tree Boosting System}},
year = {2016}
}
@article{Cowie2007,
abstract = {The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recog- nition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of struc- ture extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scien- tific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learn- ing, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem. This review is a survey of information extraction research of over two decades from these diverse communities. We},
author = {Cowie, J and Lehnert, W},
doi = {10.1561/1500000003},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cowie, Lehnert - 2007 - Information Extraction Capabilities and Challenges.pdf:pdf},
isbn = {9783540926733},
issn = {1931-7883},
journal = {Communications of the ACM},
number = {3},
pages = {261--377},
pmid = {8806820},
title = {{Information Extraction: Capabilities and Challenges}},
url = {http://dl.acm.org/citation.cfm?id=234209},
volume = {1},
year = {2007}
}
@article{Chen2015a,
abstract = {MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.},
archivePrefix = {arXiv},
arxivId = {1512.01274},
author = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
eprint = {1512.01274},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2015 - MXNet A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.pdf:pdf},
journal = {Neural Information Processing Systems, Workshop on Machine Learning Systems},
pages = {1--6},
title = {{MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1512.01274},
year = {2015}
}
@article{Chang2006,
abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Fi- nance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the sim- ple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we de- scribe the design and implementation of Bigtable.},
author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E},
doi = {10.1145/1365815.1365816},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2006 - Bigtable A distributed storage system for structured data.pdf:pdf},
isbn = {1931971471},
issn = {07342071},
journal = {7th Symposium on Operating Systems Design and Implementation (OSDI '06), November 6-8, Seattle, WA, USA},
pages = {205--218},
title = {{Bigtable: A distributed storage system for structured data}},
url = {http://research.google.com/archive/bigtable-osdi06.pdf},
year = {2006}
}
@article{Xin2014,
abstract = {From social networks to language modeling, the growing scale and importance of graph data has driven the development of numerous new graph-parallel systems (e.g., Pregel, GraphLab). By restricting the computation that can be expressed and introducing new techniques to partition and distribute the graph, these systems can efficiently execute iterative graph algorithms orders of magnitude faster than more general data-parallel systems. However, the same restrictions that enable the performance gains also make it difficult to express many of the important stages in a typical graph-analytics pipeline: constructing the graph, modifying its structure, or expressing computation that spans multiple graphs. As a consequence, existing graph analytics pipelines compose graph-parallel and data-parallel systems using external storage systems, leading to extensive data movement and complicated programming model. To address these challenges we introduce GraphX, a distributed graph computation framework that unifies graph-parallel and data-parallel computation. GraphX provides a small, core set of graph-parallel operators expressive enough to implement the Pregel and PowerGraph abstractions, yet simple enough to be cast in relational algebra. GraphX uses a collection of query optimization techniques such as automatic join rewrites to efficiently implement these graph-parallel operators. We evaluate GraphX on real-world graphs and workloads and demonstrate that GraphX achieves comparable performance as specialized graph computation systems, while outperforming them in end-to-end graph pipelines. Moreover, GraphX achieves a balance between expressiveness, performance, and ease of use.},
archivePrefix = {arXiv},
arxivId = {1402.2394},
author = {Xin, Reynold S. and Crankshaw, Daniel and Dave, Ankur and Gonzalez, Joseph E. and Franklin, Michael J. and Stoica, Ion},
eprint = {1402.2394},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xin et al. - 2014 - GraphX Unifying Data-Parallel and Graph-Parallel Analytics.pdf:pdf},
issn = {0002-9513},
journal = {The American journal of physiology},
keywords = {Acidosis,Acidosis: chemically induced,Acidosis: metabolism,Alkalosis,Animals,Epinephrine,Epinephrine: pharmacology,Heart,Heart: drug effects,Hydrogen-Ion Concentration,In Vitro Techniques,Ion Exchange,Male,Myocardium,Myocardium: metabolism,Perfusion,Potassium,Potassium: metabolism,Propranolol,Propranolol: pharmacology,Rabbits,Rats,Respiratory,Respiratory: chemically induced,Respiratory: metabolism,Sodium,Sodium: metabolism},
number = {3},
pages = {570--81},
title = {{GraphX: Unifying Data-Parallel and Graph-Parallel Analytics}},
url = {http://arxiv.org/abs/1402.2394{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/2014},
volume = {229},
year = {2014}
}
@article{Dean2012,
abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm},
archivePrefix = {arXiv},
arxivId = {fa},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, MarcAurelio Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
doi = {10.1109/ICDAR.2011.95},
eprint = {fa},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dean et al. - 2012 - Large Scale Distributed Deep Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {NIPS 2014 - Advances in Neural Information Processing Systems 27},
pages = {1--11},
pmid = {43479959},
title = {{Large Scale Distributed Deep Networks}},
year = {2012}
}
@article{Li2015,
abstract = {Distributed computing excels at processing large scale data, but the communication cost for synchronizing the shared parameters may slow down the overall performance. Fortunately, the interactions between parameter and data in many problems are sparse, which admits efficient partition in order to reduce the communication overhead. In this paper, we formulate data placement as a graph partitioning problem. We propose a distributed partitioning algorithm. We give both theoretical guarantees and a highly efficient implementation. We also provide a highly efficient implementation of the algorithm and demonstrate its promising results on both text datasets and social networks. We show that the proposed algorithm leads to 1.6x speedup of a state-of-the-start distributed machine learning system by eliminating 90$\backslash${\%} of the network communication.},
archivePrefix = {arXiv},
arxivId = {1505.04636},
author = {Li, Mu and Andersen, Dave G. and Smola, Alexander J.},
eprint = {1505.04636},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Andersen, Smola - 2015 - Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning.pdf:pdf},
journal = {arXiv:1505.04636 [cs]},
title = {{Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning}},
url = {http://arxiv.org/abs/1505.04636{\%}5Cnhttp://www.arxiv.org/pdf/1505.04636.pdf},
year = {2015}
}
@article{Lao2011,
author = {Lao, N and Mitchell, T and Cohen, WW},
file = {:E$\backslash$:/知识图谱/Random Walk Inference and Learning in A Large Scale Knowledge Base.pdf:pdf},
journal = {Proceedings of the Conference {\ldots}},
number = {March},
pages = {529--539},
title = {{Random walk inference and learning in a large scale knowledge base}},
url = {http://acl.eldoc.ub.rug.nl/mirror/D/D11/D11-1049.pdf},
year = {2011}
}
@article{Urban2016,
abstract = {Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.},
archivePrefix = {arXiv},
arxivId = {1603.05691},
author = {Urban, Gregor and Geras, Krzysztof J. and Kahou, Samira Ebrahimi and Aslan, Ozlem and Wang, Shengjie and Caruana, Rich and Mohamed, Abdelrahman and Philipose, Matthai and Richardson, Matt},
doi = {10.1038/nature14539},
eprint = {1603.05691},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Urban et al. - 2016 - Do Deep Convolutional Nets Really Need to be Deep and Convolutional.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
pmid = {26017442},
title = {{Do Deep Convolutional Nets Really Need to be Deep and Convolutional?}},
url = {http://arxiv.org/abs/1603.05691},
year = {2016}
}
@article{Bordes2013,
abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose, TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
author = {Bordes, Antoine and Usunier, Nicolas and Weston, Jason and Yakhnenko, Oksana},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes et al. - 2013 - Translating Embeddings for Modeling Multi-Relational Data.pdf:pdf},
issn = {10495258},
journal = {Advances in NIPS},
pages = {2787--2795},
pmid = {2328551},
title = {{Translating Embeddings for Modeling Multi-Relational Data}},
volume = {26},
year = {2013}
}
@article{Meng2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.06807v1},
author = {Meng, Xiangrui and Bradley, Joseph and Street, Spear and Francisco, San and Sparks, Evan and Berkeley, U C and Hall, Soda and Street, Spear and Francisco, San and Xin, Doris and Xin, Reynold and Franklin, Michael J and Berkeley, U C and Hall, Soda},
eprint = {arXiv:1505.06807v1},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meng et al. - 2016 - MLlib Machine Learning in Apache Spark.pdf:pdf},
pages = {1--7},
title = {{MLlib : Machine Learning in Apache Spark}},
volume = {17},
year = {2016}
}
@article{Verma2015,
abstract = {Google's Borg system is a cluster manager that runs hun-dreds of thousands of jobs, from many thousands of differ-ent applications, across a number of clusters each with up to tens of thousands of machines. It achieves high utilization by combining admission con-trol, efficient task-packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that min-imize fault-recovery time, and scheduling policies that re-duce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitor-ing, and tools to analyze and simulate system behavior. We present a summary of the Borg system architecture and features, important design decisions, a quantitative anal-ysis of some of its policy decisions, and a qualitative ex-amination of lessons learned from a decade of operational experience with it.},
author = {Verma, Abhishek and Pedrosa, Luis and Korupolu, Madhukar and Oppenheimer, David and Tune, Eric and Wilkes, John},
doi = {10.1145/2741948.2741964},
isbn = {9781450332385},
journal = {Proceedings of the Tenth European Conference on Computer Systems - EuroSys '15},
pages = {1--17},
title = {{Large-scale cluster management at Google with Borg}},
url = {http://dl.acm.org/citation.cfm?doid=2741948.2741964},
year = {2015}
}
@article{Akshita;2013,
author = {Akshita;, Smita;},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Akshita - 2013 - {\{}Recommender System Review{\}}.pdf:pdf},
keywords = {collaborative based,content based,data mining,hybrid recommender system,recommender system},
number = {24},
pages = {38--42},
title = {{{\{}Recommender System: Review{\}}}},
url = {http://wiki.bcmoney-mobiletv.com/index.php?title=Recommender},
volume = {71},
year = {2013}
}
@article{Dong2014,
author = {Dong, Xin Luna and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Lao, Ni and Murphy, Kevin and Strohmann, Thomas and Sun, Shaohua and Zhang, Wei},
file = {:E$\backslash$:/论文集/Knowledge Vault A Web-Scale Approach to.pdf:pdf},
isbn = {9781450329569},
keywords = {els,information extraction,knowledge bases,machine learning,probabilistic mod-},
pages = {601--610},
title = {{Knowledge Vault : A Web-Scale Approach to Probabilistic Knowledge Fusion}},
year = {2014}
}
@article{Yang2015,
abstract = {Message-oriented middleware especially for message queue has been widely used in web applications and services. Performance and scalability are quite essential in these systems however they often become the bottleneck. Existing message queues are not able to scale out elastically very well. This paper presents a decentralized distributed architecture based on peer to peer model, in which we always deliver messages with zero or one hop and take advantage of zero-copy. We implemented a scaling algorithm that can be adapted to the dynamic scale of requests and make the system scale out elastically. A series of workload tests have proved that our system can have low response latency and achieve linear increasing throughput. With these desired properties, the message system can be used to develop large scale web applications and services and provide high-performance services to users. {\textcopyright} 2014 IEEE.},
author = {Yang, Fei and Ye, Xiaojun and Zhang, Yong and Xing, Chunxiao},
doi = {10.1109/WISA.2014.38},
file = {:E$\backslash$:/知识图谱/A Decentralized Distributed Messaging.pdf:pdf},
isbn = {9781479957262},
journal = {Proceedings - 11th Web Information System and Application Conference, WISA 2014},
keywords = {High performance,Message queue,Message-oriented middleware,Peer-to-peer,Scalability},
pages = {165--171},
title = {{DZMQ: A decentralized distributed messaging system for realtime web applications and services}},
year = {2015}
}
@article{Nickel2014a,
abstract = {Tensor factorization has become a popular method for learning from multi-relational data. In this context, the rank of the factorization is an important parame-ter that determines runtime as well as generalization ability. To identify conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings, we propose a novel additive tensor factorization model to learn from latent and observable patterns on multi-relational data and present a scalable algorithm for computing the factorization. We show experimentally both that the proposed additive model does improve the predictive performance over pure latent variable methods and that it also reduces the required rank—and therefore runtime and memory complexity—significantly.},
author = {Nickel, Maximilian and Jiang, X and Tresp, V},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickel, Jiang, Tresp - 2014 - Reducing the Rank in Relational Factorization Models by Including Observable Patterns.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Reducing the Rank in Relational Factorization Models by Including Observable Patterns}},
url = {http://papers.nips.cc/paper/5448-optimistic-planning-in-markov-decision-processes-using-a-generative-model},
year = {2014}
}
@article{Lao2010,
abstract = {Scientific literature with rich metadata can be represented as a labeled directed graph. This graph representation enables a number of scientific tasks such as ad hoc retrieval or named entity recognition (NER) to be formulated as typed proximity queries in the graph. One popular proximity measure is called Random Walk with Restart (RWR), and much work has been done on the supervised learning of RWR measures by associating each edge label with a parameter. In this paper, we describe a novel learnable proximity measure which instead uses one weight per edge label sequence: proximity is defined by a weighted combination of simple "path experts", each corresponding to following a particular sequence of labeled edges. Experiments on eight tasks in two subdomains of biology show that the new learning method significantly outperforms the RWR model (both trained and untrained). We also extend the method to support two additional types of experts to model intrinsic properties of entities: query-independent experts, which generalize the PageRank measure, and popular entity experts which allow rankings to be adjusted for particular entities that are especially important. {\textcopyright} 2010 The Author(s).},
author = {Lao, Ni and Cohen, William W.},
doi = {10.1007/s10994-010-5205-8},
file = {:E$\backslash$:/知识图谱/Relational retrieval using a combination.pdf:pdf},
isbn = {9781450300551},
issn = {08856125},
journal = {Machine Learning},
keywords = {Entity relation graph,Filtering and recommending,Learning to rank,Random walk,Relational model},
number = {1},
pages = {53--67},
title = {{Relational retrieval using a combination of path-constrained random walks}},
volume = {81},
year = {2010}
}
@article{Moritz2015,
abstract = {Training deep networks is a time-consuming process, with networks for object recognition often requiring multiple days to train. For this reason, leveraging the resources of a cluster to speed up training is an important area of work. However, widely-popular batch-processing computational frameworks like MapReduce and Spark were not designed to support the asynchronous and communication-intensive workloads of existing distributed deep learning systems. We introduce SparkNet, a framework for training deep networks in Spark. Our implementation includes a convenient interface for reading data from Spark RDDs, a Scala interface to the Caffe deep learning framework, and a lightweight multi-dimensional tensor library. Using a simple parallelization scheme for stochastic gradient descent, SparkNet scales well with the cluster size and tolerates very high-latency communication. Furthermore, it is easy to deploy and use with no parameter tuning, and it is compatible with existing Caffe models. We quantify the dependence of the speedup obtained by SparkNet on the number of machines, the communication frequency, and the cluster's communication overhead, and we benchmark our system's performance on the ImageNet dataset. The code for SparkNet is available at https://github.com/amplab/SparkNet.},
archivePrefix = {arXiv},
arxivId = {1511.06051},
author = {Moritz, Philipp and Nishihara, Robert and Stoica, Ion and Jordan, Michael I.},
eprint = {1511.06051},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moritz et al. - 2015 - SparkNet Training Deep Networks in Spark.pdf:pdf},
journal = {Arxiv},
pages = {11},
title = {{SparkNet: Training Deep Networks in Spark}},
url = {http://arxiv.org/abs/1511.06051},
year = {2015}
}
@article{Bollacker2008,
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
doi = {10.1145/1376616.1376746},
file = {:E$\backslash$:/知识图谱/Freebase A Collaboratively Created Graph Database For Structuring Human Knowledge.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
journal = {SIGMOD 08 Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
pages = {1247--1250},
pmid = {3105260},
title = {{Freebase: a collaboratively created graph database for structuring human knowledge}},
url = {http://doi.acm.org/10.1145/1376616.1376746},
year = {2008}
}
@article{McMahan2013,
abstract = {Predicting ad click--through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v2},
author = {McMahan, H Brendan and Holt, Gary and Sculley, D and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and Chikkerur, Sharat and Liu, Dan and Wattenberg, Martin and Hrafnkelsson, Arnar Mar and Boulos, Tom and Kubica, Jeremy},
doi = {10.1145/2487575.2488200},
eprint = {arXiv:1301.3781v2},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McMahan et al. - 2013 - Ad click prediction a view from the trenches.pdf:pdf},
isbn = {9781450321747},
issn = {9781450321747},
journal = {Kdd},
keywords = {data mining,large-scale learning,online advertising},
pages = {1222--1230},
title = {{Ad click prediction: a view from the trenches}},
year = {2013}
}
@article{Paulheim2015,
abstract = {In the recent years, different web knowledge graphs, both free and commercial, have been created. While Google coined the term " Knowledge Graph " in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
author = {Paulheim, Heiko},
doi = {10.3233/SW-160218},
file = {:E$\backslash$:/论文集/Knowledge graph refinement.pdf:pdf},
isbn = {1099368369071},
issn = {1570-0844},
journal = {Semantic Web},
keywords = {Completion,Error Detection,Evaluation,Knowledge Graphs,Refinement},
pages = {1--0},
title = {{Knowledge Graph Refinement: A Survey of Approaches and Evaluation Methods}},
volume = {0},
year = {2015}
}
@article{Hoffart2013,
abstract = {We present YAGO2, an extension of the YAGO knowledge base, in which entities, facts, and events are anchored in both time and space. YAGO2 is built automatically from Wikipedia, GeoNames, and WordNet. It contains 447 million facts about 9.8 million entities. Human evaluation confirmed an accuracy of 95{\%} of the facts in YAGO2. In this paper, we present the extraction methodology, the integration of the spatio-temporal dimension, and our knowledge representation SPOTL, an extension of the original SPO-triple model to time and space. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Hoffart, Johannes and Suchanek, Fabian M. and Berberich, Klaus and Weikum, Gerhard},
doi = {10.1016/j.artint.2012.06.001},
file = {:E$\backslash$:/知识图谱/YAGO2 A Spatially and Temporally Enhanced Knowledge Base From Wikipedia.pdf:pdf},
isbn = {9781577356332},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {information,knowledge bases,ontologies,spatio-temporal facts},
pages = {3161--3165},
title = {{YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia}},
year = {2013}
}
@article{Burges2010,
abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very suc- cessful algorithms for solving real world ranking problems: for example an ensem- ble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and re- ports, and so here we give a self-contained, detailed and complete description of them.},
author = {Burges, Christopher J C},
doi = {10.1111/j.1467-8535.2010.01085.x},
file = {:C$\backslash$:/Users/dell/Desktop/From RankNet to LambdaRank to LambdaMART.pdf:pdf},
isbn = {0007-1013},
issn = {00071013},
journal = {Learning},
pages = {23--581},
title = {{From rankNet to LambdaRank to lambdaMART: An overview}},
volume = {11},
year = {2010}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, Christopher J C},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf},
volume = {2},
year = {1998}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedregosa et al. - 2012 - Scikit-learn Machine Learning in Python.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {{\ldots} of Machine Learning {\ldots}},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Ji2011,
abstract = {In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking -- linking names in context to entities in the KB -- and Slot Filling -- adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes ("slots") derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges.},
author = {Ji, Heng and Grishman, Ralph},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji, Grishman - 2011 - Knowledge Base Population Successful Approaches and Challenges.pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Acl},
pages = {1148--1158},
title = {{Knowledge Base Population : Successful Approaches and Challenges}},
year = {2011}
}
@article{Nickel2016,
abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.},
archivePrefix = {arXiv},
arxivId = {1503.00759},
author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
doi = {10.1109/JPROC.2015.2483592},
eprint = {1503.00759},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickel et al. - 2016 - A review of relational machine learning for knowledge graphs.pdf:pdf},
isbn = {1089801300},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Graph-based models,knowledge extraction,knowledge graphs,latent feature models,statistical relational learning},
number = {1},
pages = {11--33},
title = {{A review of relational machine learning for knowledge graphs}},
volume = {104},
year = {2016}
}
@article{Gonzalez2012,
abstract = {Large-scale graph-structured computation is central to tasks ranging from targeted advertising to natural language processing and has led to the development of several graph-parallel abstractions including Pregel and GraphLab. However, the natural graphs commonly found in the real-world have highly skewed power-law degree distributions, which challenge the assumptions made by these abstractions, limiting performance and scalability. In this paper, we characterize the challenges of computation on natural graphs in the context of existing graph-parallel abstractions. We then introduce the PowerGraph abstraction which exploits the internal structure of graph programs to address these challenges. Leveraging the PowerGraph abstraction we introduce a new approach to distributed graph placement and representation that exploits the structure of power-law graphs. We provide a detailed analysis and experimental evaluation comparing PowerGraph to two popular graph-parallel systems. Finally, we describe three different implementation strategies for PowerGraph and discuss their relative merits with empirical evaluations on large-scale real-world problems demonstrating order of magnitude gains.},
author = {Gonzalez, Je and Low, Y and Gu, H},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gonzalez, Low, Gu - 2012 - Powergraph Distributed graph-parallel computation on natural graphs.pdf:pdf},
isbn = {978-1-931971-96-6},
journal = {OSDI'12 Proceedings of the 10th USENIX conference on Operating Systems Design and Implementation},
pages = {17--30},
title = {{Powergraph: Distributed graph-parallel computation on natural graphs}},
url = {https://www.usenix.org/system/files/conference/osdi12/osdi12-final-167.pdf},
year = {2012}
}
@article{Peng2010,
abstract = {Updating an index of the web as documents are crawled requires continuously transforming a large repository of existing documents as new documents ar- rive. This task is one example of a class of data pro- cessing tasks that transform a large repository of data via small, independent mutations. These tasks lie in a gap between the capabilities of existing infrastructure. Databases do not meet the storage or throughput require- ments of these tasks: Googles indexing system stores tens of petabytes of data and processes billions of up- dates per day on thousands of machines. MapReduce and other batch-processing systems cannot process small up- dates individually as they rely on creating large batches for efficiency. We have built Percolator, a system for incrementally processing updates to a large data set, and deployed it to create the Google web search index. By replacing a batch-based indexing system with an indexing system based on incremental processing using Percolator, we process the same number of documents per day, while reducing the average age of documents in Google search results by 50{\{}{\%}{\}}.},
author = {Peng, Daniel and Dabek, Frank},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng, Dabek - 2010 - Large-scale Incremental Processing Using Distributed Transactions and Notifications.pdf:pdf},
isbn = {9781931971799},
issn = {{\textless}null{\textgreater}},
journal = {Dbms},
keywords = {Cloud computing,MapReduce},
pages = {1--15},
title = {{Large-scale Incremental Processing Using Distributed Transactions and Notifications}},
url = {http://www.usenix.org/events/osdi10/tech/full{\%}7B{\_}{\%}7Dpapers/Peng.pdf},
volume = {2006},
year = {2010}
}
@article{Yang2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.08367v1},
author = {Yang, Fan and Yang, Zhilin and Cohen, William W},
eprint = {arXiv:1702.08367v1},
file = {:E$\backslash$:/知识图谱/Differentiable Learning of Logical Rules for Knowledge Base Completion.pdf:pdf},
title = {{Differentiable Learning of Logical Rules for Knowledge Base Completion}},
year = {2016}
}
@article{Gardner2015,
abstract = {We explore some of the practicalities of using random walk inference methods, such as the Path Ranking Algorithm (PRA), for the task of knowledge base completion. We show that the random walk probabilities computed (at great expense) by PRA provide no discernible benefit to performance on this task, so they can safely be dropped. This allows us to define a simpler algorithm for generating feature matrices from graphs, which we call subgraph feature extraction (SFE). In addition to being conceptually simpler than PRA, SFE is much more efficient, reducing computation by an order of magnitude, and more expressive, allowing for much richer features than paths between two nodes in a graph. We show experimentally that this technique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.3044v2},
author = {Gardner, Matt and Mitchell, Tom},
eprint = {arXiv:1402.3044v2},
file = {:E$\backslash$:/论文集/Efficient and Expressive Knowledge Base Completion.pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of EMNLP},
number = {September},
pages = {1488--1498},
title = {{Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction}},
year = {2015}
}
@article{Niu2012,
abstract = {We present an end-to-end (live) demonstration system called DeepDive that performs knowledge-base construction (KBC) from hundreds of millions of web pages. DeepDive employs statistical learning and inference to combine diverse data resources and best-of-breed algorithms. A key challenge of this approach is scalability, i.e., how to deal with terabytes of imperfect data efficiently. We describe how we address the scalability challenges to achieve web-scale KBC and the lessons we have learned from building DeepDive.},
author = {Niu, Feng and Zhang, Ce and R{\'{e}}, Christopher and Shavlik, Jude},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Niu et al. - 2012 - DeepDive Web-scale knowledge-base construction using statistical learning and inference.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {25--28},
title = {{DeepDive: Web-scale knowledge-base construction using statistical learning and inference}},
volume = {884},
year = {2012}
}
@article{Lao2012,
author = {Lao, Ni and Mitamura, Teruko and Mitchell, Tom and Technologies, Information},
file = {:E$\backslash$:/论文集/Efficient Random Walk Inference with knowledge graph.pdf:pdf},
journal = {PhD thesis},
title = {{Efficient Random Walk Inference with Knowledge Bases}},
year = {2012}
}
@article{Garcia-Molina2011,
abstract = {How to address user information needs amidst a preponderance of data.},
author = {Garcia-Molina, Hector and Koutrika, Georgia and Parameswaran, Aditya},
doi = {10.1145/2018396.2018423},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcia-Molina, Koutrika, Parameswaran - 2011 - Information seeking convergence of search, recommendations, and advertising.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {121--130},
title = {{Information seeking: convergence of search, recommendations, and advertising}},
url = {http://dl.acm.org/citation.cfm?doid=2018396.2018423},
volume = {54},
year = {2011}
}
@article{Wang2015a,
abstract = {Recently, several large-scale RDF knowledge bases have been built and applied in many knowledge-based applications. To further increase the number of facts in RDF knowledge bases, logic rules can be used to predict new facts based on the existing ones. Therefore, how to automatically learn reliable rules from large-scale knowledge bases becomes increasingly important. In this paper, we propose a novel rule learning approach named RDF2Rules for RDF knowledge bases. RDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting frequent patterns in knowledge bases, and then generates rules from the mined FPCs. Because each FPC can produce multiple rules, and effective pruning strategy is used in the process of mining FPCs, RDF2Rules works very efficiently. Another advantage of RDF2Rules is that it uses the entity type information when generates and evaluates rules, which makes the learned rules more accurate. Experiments show that our approach outperforms the compared approach in terms of both efficiency and accuracy.},
archivePrefix = {arXiv},
arxivId = {1512.07734},
author = {Wang, Zhichun and Li, Juanzi},
eprint = {1512.07734},
file = {:E$\backslash$:/论文集/RDF2Rules Learning Rules from RDF Knowledge Bases.pdf:pdf},
journal = {arXiv:1512.07734 [cs]},
title = {{RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent Predicate Cycles}},
url = {http://arxiv.org/abs/1512.07734{\%}5Cnhttp://www.arxiv.org/pdf/1512.07734.pdf},
year = {2015}
}
@article{Wu2015,
abstract = {Gram is distributed system that runs on top of the FaRM threading and RPC model. It uses the same data layout as Grace, it uses messaging passing like FaRM but can combine the number of queues into 1 per numa node for message batching reasons. Its messaging and layout of data is numa aware like Grace.It also uses RDMA and intelligently combines waiting for results of remote RDMA requests with other workload and is able to have the CPU as the bottle neck. This paper only analyzes large batch workloads and does not look at point queries. Note: This looks to be a multithreaded extention of the grace paper.},
author = {Wu, Ming and Yang, Fan and Xue, Jilong and Xiao, Wencong and Miao, Youshan and Wei, Lan and Lin, Haoxiang and Dai, Yafei and Zhou, Lidong},
doi = {10.1145/2806777.2806849},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2015 - Gram.pdf:pdf},
isbn = {9781450336512},
journal = {Proceedings of the Sixth ACM Symposium on Cloud Computing - SoCC '15},
keywords = {graph computation engine,rdma,scalability},
number = {July},
pages = {408--421},
title = {{GRAM: Scaling Graph Computation to the Trillions}},
url = {http://dl.acm.org/citation.cfm?doid=2806777.2806849},
year = {2015}
}
@article{Cheng2016,
abstract = {Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide {\&} Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide {\&} Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.},
archivePrefix = {arXiv},
arxivId = {1606.07792},
author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
doi = {10.1145/2988450.2988454},
eprint = {1606.07792},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2016 - Wide {\&} Deep Learning for Recommender Systems.pdf:pdf},
isbn = {9781450347952},
keywords = {deep learning,recommender systems,wide},
pages = {1--4},
title = {{Wide {\&} Deep Learning for Recommender Systems}},
url = {http://arxiv.org/abs/1606.07792},
year = {2016}
}
@article{Shin2015,
abstract = {We introduce the notion of a local shadow for a black hole and determine its shape for the particular case of a distorted Schwarzschild black hole. Considering the lowest-order even and odd multiple moments, we compute the relation between the deformations of the shadow of a Schwarzschild black hole and the distortion multiple moments. For the range of values of multiple moments that we consider, the horizon is deformed much less than its corresponding shadow, suggesting the horizon is more `rigid'. Quite unexpectedly we find that a prolate distortion of the horizon gives rise to an oblate distortion of the shadow, and vice-versa.},
archivePrefix = {arXiv},
arxivId = {1502.0073},
author = {Shin, Jaeho and Wu, Sen and Wang, Feiran and {De Sa}, Christopher and Zhang, Ce and R{\'{e}}, Christopher},
doi = {10.14778/2809974.2809991},
eprint = {1502.0073},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shin et al. - 2015 - Incremental knowledge base construction using DeepDive.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {11},
pages = {1310--1321},
title = {{Incremental knowledge base construction using DeepDive}},
url = {http://dl.acm.org/citation.cfm?doid=2809974.2809991},
volume = {8},
year = {2015}
}
@article{Covington2016,
abstract = {YouTube represents one of the largest scale and most sophis-ticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and fo-cus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a sepa-rate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintain-ing a massive recommendation system with enormous user-facing impact.},
author = {Covington, Paul and Adams, Jay and Sargin, Emre},
doi = {10.1145/2959100.2959190},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Covington, Adams, Sargin - 2016 - Deep Neural Networks for YouTube Recommendations.pdf:pdf},
isbn = {9781450340359},
journal = {Proceedings of the 10th ACM Conference on Recommender Systems - RecSys '16},
keywords = {deep learning,recommender system,scalability},
pages = {191--198},
title = {{Deep Neural Networks for YouTube Recommendations}},
url = {http://dl.acm.org/citation.cfm?doid=2959100.2959190},
year = {2016}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Nips},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}
@article{Li2014,
abstract = {We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance. To demonstrate the scalability of the proposed frame- work, we show experimental results on petabytes of real data with billions of examples and parameters on prob- lems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Li, M and Andersen, Dg and Park, Jw},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Andersen, Park - 2014 - Scaling distributed machine learning with the parameter server.pdf:pdf},
isbn = {9781931971164},
issn = {10495258},
journal = {11th USENIX Symposium on Operating Systems Design and Implementation},
pages = {17},
pmid = {15991970},
title = {{Scaling distributed machine learning with the parameter server}},
url = {https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li{\_}mu.pdf},
year = {2014}
}
@article{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:pdf},
isbn = {9781450330633},
issn = {10636919},
journal = {ACM International Conference on Multimedia},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
pages = {675--678},
pmid = {18267787},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
url = {http://arxiv.org/abs/1408.5093},
year = {2014}
}
@article{Wang2014a,
abstract = {Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art.},
archivePrefix = {arXiv},
arxivId = {1409.2944},
author = {Wang, Hao and Wang, Naiyan and Yeung, Dit-Yan},
doi = {10.1145/2783258.2783273},
eprint = {1409.2944},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Wang, Yeung - 2014 - Collaborative Deep Learning for Recommender Systems.pdf:pdf;:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Wang, Yeung - 2014 - Collaborative Deep Learning for Recommender Systems(2).pdf:pdf;:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Wang, Yeung - 2014 - Collaborative Deep Learning for Recommender Systems(3).pdf:pdf},
isbn = {9781450336642},
issn = {9781450336642},
keywords = {deep learning,recommender systems,text,topic model},
title = {{Collaborative Deep Learning for Recommender Systems}},
url = {http://arxiv.org/abs/1409.2944},
year = {2014}
}
@article{Cao2007,
abstract = {The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as ‘instances' in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as ‘instances' in learning. The paper proposes a new probabilistic method for the approach. Specifically it introduces two probability models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on infor- mation retrieval show that the proposed listwise approach performs better than the pairwise ap- proach},
author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
doi = {10.1145/1273496.1273513},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2007 - Learning to Rank From Pairwise Approach to Listwise Approach.pdf:pdf},
isbn = {9781595937933},
issn = {1595937935},
journal = {Proceedings of the 24th international conference on Machine learning},
pages = {129--136},
title = {{Learning to Rank : From Pairwise Approach to Listwise Approach}},
url = {http://research.microsoft.com/en-us/people/tyliu/listnet.pdf},
year = {2007}
}
@article{Lao2015a,
author = {Lao, N and Minkov, E and Cohen, Ww},
journal = {Cs.Cmu.Edu},
keywords = {knowledge-base,logical inference,random walk},
pages = {666--675},
title = {{Learning Relational Features with Backward Random Walks}},
url = {http://www.cs.cmu.edu/{~}nlao/doc/ecml2012.pdf},
year = {2015}
}
@article{Kyrola2012,
abstract = {Current systems for graph computation require a distributed computing cluster to handle very large real-world problems, such as analysis on social networks or the web graph. While distributed computational resources have become more accessible, developing distributed graph algorithms still remains challenging, especially to non-experts. In this work, we present GraphChi, a disk-based system for computing efficiently on graphs with billions of edges. By using a well-known method to break large graphs into small parts, and a novel parallel sliding windows method, GraphChi is able to execute several advanced data mining, graph mining, and machine learning algorithms on very large graphs, using just a single consumer-level computer. We further extend GraphChi to support graphs that evolve over time, and demonstrate that, on a single computer, GraphChi can process over one hundred thousand graph updates per second, while simultaneously performing computation. We show, through experiments and theoretical analysis, that GraphChi performs well on both SSDs and rotational hard drives. By repeating experiments reported for existing distributed systems, we show that, with only fraction of the resources, GraphChi can solve the same problems in very reasonable time. Our work makes large-scale graph computation available to anyone with a modern PC.},
author = {Kyrola, Aapo and Blelloch, Guy and Guestrin, Carlos},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kyrola, Blelloch, Guestrin - 2012 - GraphChi Large-Scale Graph Computation on Just a PC Disk-based Graph Computation.pdf:pdf},
isbn = {978-1-931971-96-6},
issn = {1432-4350},
journal = {Proceedings of the 10th USENIX conference on Operating Systems Design and Implementation},
pages = {31--46},
title = {{GraphChi: Large-Scale Graph Computation on Just a PC Disk-based Graph Computation}},
year = {2012}
}
@article{Fikes2010,
abstract = {Google},
author = {Fikes, Andrew},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fikes - 2010 - Google Storage architecture and challenges.pdf:pdf},
journal = {Talk at the Google Faculty Summit},
title = {{Google Storage architecture and challenges}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Storage+Architecture+and+Challenges{\#}0},
year = {2010}
}
@article{Yin2016,
abstract = {Search engines play a crucial role in our daily lives. Relevance is the core problem of a commercial search engine. It has attracted thousands of researchers from both academia and industry and has been studied for decades. Relevance in a modern search engine has gone far beyond text matching, and now involves tremendous challenges. The semantic gap between queries and URLs is the main barrier for improving base relevance. Clicks help provide hints to improve relevance, but unfortunately for most tail queries, the click information is too sparse, noisy, or missing entirely. For comprehensive relevance, the recency and location sensitivity of results is also critical. In this paper, we give an overview of the solutions for relevance in the Yahoo search engine. We introduce three key techniques for base relevance – ranking functions, semantic matching features and query rewriting. We also describe solutions for recency sensitive relevance and location sensitive relevance. This work builds upon 20 years of existing efforts on Yahoo search, summarizes the most recent advances and provides a series of practical relevance solutions. The reported performance is based on Yahoo's commercial search engine, where tens of billions of URLs are indexed and served by the ranking system. Download PDF},
archivePrefix = {arXiv},
arxivId = {hep-th/0010225v1},
author = {Yin, Dawei and Nobata, Chikashi and Langlois, Jean-Marc and Chang, Yi and Hu, Yuening and Tang, Jiliang and Daly, Tim and Zhou, Mianwei and Ouyang, Hua and Chen, Jianhui and Kang, Changsung and Deng, Hongbo},
doi = {10.1145/2939672.2939677},
eprint = {0010225v1},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yin et al. - 2016 - Ranking Relevance in Yahoo Search.pdf:pdf},
isbn = {9781450342322},
issn = {1046-8188},
journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
keywords = {deep learning,learning to rank,query rewriting,semantic matching},
pages = {323--332},
pmid = {9849112},
primaryClass = {hep-th},
title = {{Ranking Relevance in Yahoo Search}},
url = {http://dl.acm.org/citation.cfm?doid=2939672.2939677},
year = {2016}
}
@article{Li2006,
author = {Li, Ping and Burges, Christopher J C and Research, Microsoft and Corporation, Microsoft and Wu, Qiang},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2006 - Learning to Rank Using Classi cation and Gradient Boosting.pdf:pdf},
journal = {ReCALL},
title = {{Learning to Rank Using Classi cation and Gradient Boosting}},
year = {2006}
}
@article{Yang2015a,
abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2{\%} vs. 54.7{\%} by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
archivePrefix = {arXiv},
arxivId = {1412.6575},
author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
eprint = {1412.6575},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2015 - Embedding Entities and Relations for Learning and Inference in Knowledge Bases.pdf:pdf},
journal = {Iclr},
pages = {12},
title = {{Embedding Entities and Relations for Learning and Inference in Knowledge Bases}},
url = {http://arxiv.org/abs/1412.6575},
year = {2015}
}
@article{Ren2014,
abstract = {The problem of learning user search intents has attracted intensive attention from both industry and academia. How- ever, state-of-the-art intent learning algorithms suffer from different drawbacks when only using a single type of da- ta source. For example, query text has difficulty in distin- guishing ambiguous queries; search log is bias to the order of search results and users' noisy click behaviors. In this work, we for the first time leverage three types of objects, namely queries, web pages and Wikipedia concepts collaboratively for learning generic search intents and construct a hetero- geneous graph to represent multiple types of relationships between them. A novel unsupervised method called hetero- geneous graph-based soft-clustering is developed to derive an intent indicator for each object based on the constructed het- erogeneous graph. With the proposed co-clustering method, one can enhance the quality of intent understanding by tak- ing advantage of different types of data, which complement each other, and make the implicit intents easier to interpret with explicit knowledge from Wikipedia concepts. Experi- ments on two real-world datasets demonstrate the power of the proposed method where it achieves a 9.25{\%} improve- ment in terms of NDCG on search ranking task and a 4.67{\%} enhancement in terms of Rand index on object co-clustering task compared to the best state-of-the-art method.},
author = {Ren, Xiang and Wang, Yujing and Yu, Xiao and Yan, Jun and Chen, Zheng and Han, Jiawei},
doi = {10.1145/2556195.2556222},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2014 - Heterogeneous graph-based intent learning with queries, web pages and Wikipedia concepts.pdf:pdf},
isbn = {9781450323512},
journal = {Proceedings of the 7th ACM international conference on Web search and data mining - WSDM '14},
keywords = {author was,done when the first,heterogeneous graph clustering,search intent,this work was partially,wikipedia},
pages = {23--32},
title = {{Heterogeneous graph-based intent learning with queries, web pages and Wikipedia concepts}},
url = {http://dl.acm.org/citation.cfm?doid=2556195.2556222},
year = {2014}
}
@article{Socher2013,
abstract = {Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their dis- crete entities and relationships. In this paper we introduce an expressive neu- ral tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We showthat performance can be improved when en- tities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the “Sumatran tiger” and “Bengal tiger.” Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2{\%} and 90.0{\%}, respectively.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3618v2},
author = {Socher, Richard and Chen, Danqi and Manning, Christopher D. and Ng, Andrew Y.},
eprint = {arXiv:1301.3618v2},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher et al. - 2013 - Reasoning With Neural Tensor Networks for Knowledge Base Completion.pdf:pdf},
issn = {10495258},
journal = {Proceedings of the Advances in Neural Information Processing Systems 26 (NIPS 2013)},
pages = {1--10},
title = {{Reasoning With Neural Tensor Networks for Knowledge Base Completion}},
year = {2013}
}
@article{Rastogi2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.04672v1},
author = {Rastogi, Pushpendre and Durme, Benjamin Van},
eprint = {arXiv:1605.04672v1},
file = {:E$\backslash$:/论文集/A Critical Examination of RESCAL for Completion of Knowledge Bases with Transitive Relations.pdf:pdf},
number = {October 2013},
title = {{A Critical Examination of RESCAL for Completion of Knowledge Bases}},
year = {2014}
}
@article{Burrows2006,
abstract = {We describe our experiences with the Chubby lock service, which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. Chubby provides an interface much like a distributed file system with advisory locks, but the design emphasis is on availability and reliability, as opposed to high performance. Many instances of the service have been used for over a year, with several of them each handling a few tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modified to accommodate the differences.},
author = {Burrows, Mike},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burrows - 2006 - The Chubby lock service for loosely-coupled distributed systems.pdf:pdf},
isbn = {1-931971-47-1},
issn = {{\textless}null{\textgreater}},
journal = {OSDI '06: Proceedings of the 7th symposium on Operating systems design and implementation SE - OSDI '06},
keywords = {consensus,distributed-systems,fault-tolerance},
pages = {335--350},
title = {{The Chubby lock service for loosely-coupled distributed systems}},
url = {citeulike-article-id:6502774{\%}5Cnhttp://portal.acm.org/citation.cfm?id=1298487},
year = {2006}
}
@article{Model2009,
author = {Model, Data},
doi = {10.1109/ICDE.2010.5447738},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Model - 2009 - Hive - A Petabyte Scale Data Warehouse using Hadoop(2).pdf:pdf},
isbn = {9781424454464},
keywords = {[Electronic Manuscript]},
pages = {1--5},
title = {{Hive - A Petabyte Scale Data Warehouse using Hadoop}},
year = {2009}
}
@article{Dong,
author = {Dong, Xin Luna and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Murphy, Kevin and Sun, Shaohua and Zhang, Wei},
file = {:E$\backslash$:/论文集/From Data Fusion to Knowledge Fusion.pdf:pdf},
pages = {881--892},
title = {{From Data Fusion to Knowledge Fusion}}
}
@article{Shute2013,
abstract = {F1 is a distributed relational database system built at Google to support the AdWords business. F1 is a hybrid database that combines high availability, the scalability of NoSQL systems like Bigtable, and the consistency and usability of traditional SQL databases. F1 is built on Spanner, which provides synchronous cross-datacenter replication and strong consistency. Synchronous replication implies higher commit latency, but we mitigate that latency by using a hierarchical schema model with structured data types and through smart application design. F1 also includes a fully functional distributed SQL query engine and automatic change tracking and publishing.},
author = {Shute, Jeff and Vingralek, R and Samwel, Bart and Rae, Ian},
doi = {10.14778/2536222.2536232},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shute et al. - 2013 - F1 A distributed SQL database that scales.pdf:pdf},
isbn = {9781450312479},
issn = {2150-8097},
journal = {Proceedings of the {\ldots}},
number = {11},
pages = {1068--1079},
title = {{F1: A distributed SQL database that scales}},
url = {http://dl.acm.org/citation.cfm?id=2536232},
volume = {6},
year = {2013}
}
@article{Lao2015,
author = {Lao, N and Minkov, E and Cohen, Ww},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lao, Minkov, Cohen - 2015 - Learning Relational Features with Backward Random Walks.pdf:pdf},
isbn = {9781941643723},
journal = {Cs.Cmu.Edu},
keywords = {knowledge-base,logical inference,random walk},
pages = {666--675},
title = {{Learning Relational Features with Backward Random Walks}},
url = {http://www.cs.cmu.edu/{~}nlao/doc/ecml2012.pdf},
year = {2015}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Xin2015,
author = {Xin, Xin and Liu, Zhirun and Lin, Chin-yew and Huang, Heyan and Wei, Xiaochi and Guo, Ping},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xin et al. - 2015 - Cross-Domain Collaborative Filtering with Review Text.pdf:pdf},
keywords = {Technical Papers — Recommender Systems},
number = {Ijcai},
pages = {1827--1833},
title = {{Cross-Domain Collaborative Filtering with Review Text}},
year = {2015}
}
@article{Ongaro2014,
author = {Ongaro, Diego and Ousterhout, John},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ongaro, Ousterhout - 2014 - Raft (1).pdf:pdf},
title = {{Raft (1)}},
year = {2014}
}
@article{Zaharia2010,
abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However; Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs; and can be used to interactively query a 39 GB dataset with sub-second response time.; as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals; most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms},
author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
doi = {10.1007/s00256-009-0861-0},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaharia et al. - 2010 - Spark Cluster Computing with Working Sets.pdf:pdf},
isbn = {1432-2161 (Electronic)$\backslash$n0364-2348 (Linking)},
issn = {03642348},
journal = {HotCloud'10 Proceedings of the 2nd USENIX conference on Hot topics in cloud computing},
pages = {10},
pmid = {20205351},
title = {{Spark : Cluster Computing with Working Sets}},
year = {2010}
}
@article{Ahmed2013,
abstract = {Natural graphs, such as social networks, email graphs, or instant messaging patterns, have become pervasive through the internet. These graphs are massive, often containing hundreds of millions of nodes and billions of edges. While some theoretical models have been proposed to study such graphs, their analysis is still difficult due to the scale and nature of the data. We propose a framework for large-scale graph decomposition and inference. To resolve the scale, our framework is distributed so that the data are partitioned over a shared-nothing set of machines. We propose a novel factorization technique that relies on partitioning a graph so as to minimize the number of neighboring vertices rather than edges across partitions. Our decomposition is based on a streaming algorithm. It is network-aware as it adapts to the network topology of the underlying computational hardware. We use local copies of the variables and an efficient asynchronous communication protocol to synchronize the replicated values in order to perform most of the computation without having to incur the cost of network communication. On a graph of 200 million vertices and 10 billion edges, derived from an email communication network, our algorithm retains convergence properties while allowing for almost linear scalability in the number of computers.},
author = {Ahmed, Amr and Shervashidze, Nino and Narayanamurthy, Shravan and Josifovski, Vanja and Smola, Alexander J.},
doi = {10.1145/2488388.2488393},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahmed et al. - 2013 - Distributed Large-scale Natural Graph Factorization.pdf:pdf},
isbn = {978-1-4503-2035-1},
journal = {Proceedings of the 22Nd International Conference on World Wide Web},
keywords = {asynchronous algorithms,distributed optimization,graph algorithms,graph factorization,large-scale machine learning,matrix factorization},
pages = {37--48},
pmid = {957896},
title = {{Distributed Large-scale Natural Graph Factorization}},
url = {http://dl.acm.org/citation.cfm?id=2488388.2488393},
year = {2013}
}
@article{Banerjee,
archivePrefix = {arXiv},
arxivId = {1507.05998},
author = {Banerjee, Siddhartha and Lofgren, Peter},
eprint = {1507.05998},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Banerjee, Lofgren - Unknown - Fast Bidirectional Probability Estimation in Markov Models.pdf:pdf},
issn = {10495258},
pages = {1--9},
title = {{Fast Bidirectional Probability Estimation in Markov Models}}
}
@article{Xiong2015,
author = {Xiong, Wenhan and Hoang, Thien and Wang, William Yang},
file = {:E$\backslash$:/研究生毕业论文/DeepPath.pdf:pdf},
title = {{DeepPath : A Reinforcement Learning Method for Knowledge Graph Reasoning}},
year = {2015}
}
@article{Bordes2009,
author = {Bordes, Antoine and Weston, Jason},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes, Weston - 2009 - Learning Structured Embeddings of Knowledge Bases.pdf:pdf},
number = {Bengio},
title = {{Learning Structured Embeddings of Knowledge Bases}},
year = {2009}
}
@misc{Manning2009,
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar},
booktitle = {Online},
chapter = {Boolean Re},
doi = {10.1109/LPT.2009.2020494},
editor = {Salas, A Cannon-Bowers E},
eprint = {0521865719 9780521865715},
institution = {Cambridge University Press Cambridge, England},
isbn = {0521865719},
issn = {13864564},
keywords = {keyword},
pages = {1},
pmid = {10575050},
publisher = {Cambridge University Press},
title = {{An Introduction to Information Retrieval}},
url = {http://dspace.cusat.ac.in/dspace/handle/123456789/2538},
year = {2009}
}
@article{Gardner2014,
abstract = {Abstract Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has ...},
author = {Gardner, Matt and Talukdar, Partha P. and Krishnamurthy, Jayant and Mitchell, Tom M.},
file = {:E$\backslash$:/论文集/Incorporating Vector Space Similarity in Random Walk Inference over.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {397--406},
title = {{Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases}},
year = {2014}
}
@article{Sharma2016a,
author = {Sharma, Aneesh and Jiang, Jerry and Bommannavar, Praveen and Larson, Brian and Lin, Jimmy},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sharma et al. - 2016 - GraphJet Real-Time Content Recommendations at Twitter.pdf:pdf},
number = {10},
pages = {1281--1292},
title = {{GraphJet : Real-Time Content Recommendations at Twitter}},
volume = {9},
year = {2016}
}
@article{Samadi2015,
abstract = {Recently, several Web-scale knowledge harvesting systems have been built, each of which is competent at extracting information from certain types of data (e.g., unstructured text, structured tables on the web, etc.). In order to determine the response to a new query posed to such systems (e.g., is sugar a healthy food?), it is useful to integrate opinions from multiple systems. If a response is desired within a specific time budget (e.g., in less than 2 seconds), then maybe only a subset of these resources can be queried. In this paper, we address the problem of knowledge integration for on-demand time-budgeted query answering. We propose a new method, AskWorld, which learns a policy that chooses which queries to send to which resources, by accommodating varying budget constraints that are available only at query (test) time. Through extensive experiments on real world datasets, we demonstrate AskWorld's capability in selecting most informative resources to query within test-time constraints, resulting in improved performance compared to competitive baselines.},
author = {Samadi, Mehdi and Talukdar, Partha and Veloso, Manuela and Mitchell, Tom},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samadi et al. - 2015 - AskWorld Budget-sensitive query evaluation for knowledge-on-demand.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {837--843},
title = {{AskWorld: Budget-sensitive query evaluation for knowledge-on-demand}},
volume = {2015-Janua},
year = {2015}
}
@article{Toshniwal2014,
abstract = {This paper describes the use of Storm at Twitter. Storm is a real- time fault-tolerant and distributed stream data processing system. Storm is currently being used to run various critical computations in Twitter at scale, and in real-time. This paper describes the architecture of Storm and its methods for distributed scale-out and fault-tolerance. This paper also describes how queries (aka. topologies) are executed in Storm, and presents some operational stories based on running Storm at Twitter. We also present results from an empirical evaluation demonstrating the resilience of Storm in dealing with machine failures. Storm is under active development at Twitter and we also present some potential directions for future work.},
author = {Toshniwal, Ankit and Donham, Jake and Bhagat, Nikunj and Mittal, Sailesh and Ryaboy, Dmitriy and Taneja, Siddarth and Shukla, Amit and Ramasamy, Karthik and Patel, Jignesh M. and Kulkarni, Sanjeev and Jackson, Jason and Gade, Krishna and Fu, Maosong},
doi = {10.1145/2588555.2595641},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Toshniwal et al. - 2014 - Storm@twitter(5).pdf:pdf},
isbn = {9781450323765},
journal = {Proceedings of the 2014 ACM SIGMOD international conference on Management of data - SIGMOD '14},
keywords = {real-time query processing,stream data management},
pages = {147--156},
title = {{Storm@twitter}},
url = {http://dl.acm.org/citation.cfm?id=2588555.2595641},
year = {2014}
}
