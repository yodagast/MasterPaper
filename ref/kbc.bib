@article{Xiong2015,
author = {Xiong, Wenhan and Hoang, Thien and Wang, William Yang},
file = {:E$\backslash$:/研究生毕业论文/DeepPath.pdf:pdf},
title = {{DeepPath : A Reinforcement Learning Method for Knowledge Graph Reasoning}},
year = {2015}
}
@article{Yang2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.08367v1},
author = {Yang, Fan and Yang, Zhilin and Cohen, William W},
eprint = {arXiv:1702.08367v1},
file = {:E$\backslash$:/知识图谱/Differentiable Learning of Logical Rules for Knowledge Base Completion.pdf:pdf},
title = {{Differentiable Learning of Logical Rules for Knowledge Base Completion}},
year = {2016}
}
@article{Cohen2016,
abstract = {Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into "deep" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are "unrolled" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1605.06523},
author = {Cohen, William W.},
eprint = {1605.06523},
file = {:E$\backslash$:/知识图谱/TensorLog A Differentiable Deductive Database.pdf:pdf},
number = {Nips},
title = {{TensorLog: A Differentiable Deductive Database}},
url = {http://arxiv.org/abs/1605.06523},
year = {2016}
}
@article{Nickel2011,
abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickel, Tresp, Kriegel - 2011 - A Three-Way Model for Collective Learning on Multi-Relational Data.pdf:pdf;:E$\backslash$:/知识图谱/A Three-Way Model for Collective Learning on Multi-Relational Data.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {28th International Conference on Machine Learning},
pages = {809----816},
title = {{A Three-Way Model for Collective Learning on Multi-Relational Data}},
year = {2011}
}
@article{Lao2010,
abstract = {Scientific literature with rich metadata can be represented as a labeled directed graph. This graph representation enables a number of scientific tasks such as ad hoc retrieval or named entity recognition (NER) to be formulated as typed proximity queries in the graph. One popular proximity measure is called Random Walk with Restart (RWR), and much work has been done on the supervised learning of RWR measures by associating each edge label with a parameter. In this paper, we describe a novel learnable proximity measure which instead uses one weight per edge label sequence: proximity is defined by a weighted combination of simple "path experts", each corresponding to following a particular sequence of labeled edges. Experiments on eight tasks in two subdomains of biology show that the new learning method significantly outperforms the RWR model (both trained and untrained). We also extend the method to support two additional types of experts to model intrinsic properties of entities: query-independent experts, which generalize the PageRank measure, and popular entity experts which allow rankings to be adjusted for particular entities that are especially important. {\textcopyright} 2010 The Author(s).},
author = {Lao, Ni and Cohen, William W.},
doi = {10.1007/s10994-010-5205-8},
file = {:E$\backslash$:/知识图谱/Relational retrieval using a combination.pdf:pdf},
isbn = {9781450300551},
issn = {08856125},
journal = {Machine Learning},
keywords = {Entity relation graph,Filtering and recommending,Learning to rank,Random walk,Relational model},
number = {1},
pages = {53--67},
title = {{Relational retrieval using a combination of path-constrained random walks}},
volume = {81},
year = {2010}
}
@article{Bordes2009,
author = {Bordes, Antoine and Weston, Jason},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes, Weston - 2009 - Learning Structured Embeddings of Knowledge Bases.pdf:pdf},
number = {Bengio},
title = {{Learning Structured Embeddings of Knowledge Bases}},
year = {2009}
}
@article{Nickel2014a,
abstract = {Tensor factorization has become a popular method for learning from multi-relational data. In this context, the rank of the factorization is an important parame-ter that determines runtime as well as generalization ability. To identify conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings, we propose a novel additive tensor factorization model to learn from latent and observable patterns on multi-relational data and present a scalable algorithm for computing the factorization. We show experimentally both that the proposed additive model does improve the predictive performance over pure latent variable methods and that it also reduces the required rank—and therefore runtime and memory complexity—significantly.},
author = {Nickel, Maximilian and Jiang, X and Tresp, V},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickel, Jiang, Tresp - 2014 - Reducing the Rank in Relational Factorization Models by Including Observable Patterns.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Reducing the Rank in Relational Factorization Models by Including Observable Patterns}},
url = {http://papers.nips.cc/paper/5448-optimistic-planning-in-markov-decision-processes-using-a-generative-model},
year = {2014}
}
@article{Niu2012,
abstract = {We present an end-to-end (live) demonstration system called DeepDive that performs knowledge-base construction (KBC) from hundreds of millions of web pages. DeepDive employs statistical learning and inference to combine diverse data resources and best-of-breed algorithms. A key challenge of this approach is scalability, i.e., how to deal with terabytes of imperfect data efficiently. We describe how we address the scalability challenges to achieve web-scale KBC and the lessons we have learned from building DeepDive.},
author = {Niu, Feng and Zhang, Ce and R{\'{e}}, Christopher and Shavlik, Jude},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Niu et al. - 2012 - DeepDive Web-scale knowledge-base construction using statistical learning and inference.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {25--28},
title = {{DeepDive: Web-scale knowledge-base construction using statistical learning and inference}},
volume = {884},
year = {2012}
}
@article{Shin2015,
abstract = {We introduce the notion of a local shadow for a black hole and determine its shape for the particular case of a distorted Schwarzschild black hole. Considering the lowest-order even and odd multiple moments, we compute the relation between the deformations of the shadow of a Schwarzschild black hole and the distortion multiple moments. For the range of values of multiple moments that we consider, the horizon is deformed much less than its corresponding shadow, suggesting the horizon is more `rigid'. Quite unexpectedly we find that a prolate distortion of the horizon gives rise to an oblate distortion of the shadow, and vice-versa.},
archivePrefix = {arXiv},
arxivId = {1502.0073},
author = {Shin, Jaeho and Wu, Sen and Wang, Feiran and {De Sa}, Christopher and Zhang, Ce and R{\'{e}}, Christopher},
doi = {10.14778/2809974.2809991},
eprint = {1502.0073},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shin et al. - 2015 - Incremental knowledge base construction using DeepDive.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {11},
pages = {1310--1321},
title = {{Incremental knowledge base construction using DeepDive}},
url = {http://dl.acm.org/citation.cfm?doid=2809974.2809991},
volume = {8},
year = {2015}
}
@article{Wang2015a,
abstract = {Recently, several large-scale RDF knowledge bases have been built and applied in many knowledge-based applications. To further increase the number of facts in RDF knowledge bases, logic rules can be used to predict new facts based on the existing ones. Therefore, how to automatically learn reliable rules from large-scale knowledge bases becomes increasingly important. In this paper, we propose a novel rule learning approach named RDF2Rules for RDF knowledge bases. RDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting frequent patterns in knowledge bases, and then generates rules from the mined FPCs. Because each FPC can produce multiple rules, and effective pruning strategy is used in the process of mining FPCs, RDF2Rules works very efficiently. Another advantage of RDF2Rules is that it uses the entity type information when generates and evaluates rules, which makes the learned rules more accurate. Experiments show that our approach outperforms the compared approach in terms of both efficiency and accuracy.},
archivePrefix = {arXiv},
arxivId = {1512.07734},
author = {Wang, Zhichun and Li, Juanzi},
eprint = {1512.07734},
file = {:E$\backslash$:/论文集/RDF2Rules Learning Rules from RDF Knowledge Bases.pdf:pdf},
journal = {arXiv:1512.07734 [cs]},
title = {{RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent Predicate Cycles}},
url = {http://arxiv.org/abs/1512.07734{\%}5Cnhttp://www.arxiv.org/pdf/1512.07734.pdf},
year = {2015}
}
@inproceedings{Wang2016,
author = {Wang, Quan and Liu, Jing and Luo, Yuanfei and Wang, Bin and Lin, Chin-yew},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
file = {:E$\backslash$:/论文集/acl2016{\_}camera-ready.pdf:pdf},
isbn = {9781510827585},
pages = {1308--1318},
title = {{Knowledge base completion via coupled path ranking}},
year = {2016}
}
@article{Rastogi2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.04672v1},
author = {Rastogi, Pushpendre and Durme, Benjamin Van},
eprint = {arXiv:1605.04672v1},
file = {:E$\backslash$:/论文集/A Critical Examination of RESCAL for Completion of Knowledge Bases with Transitive Relations.pdf:pdf},
number = {October 2013},
title = {{A Critical Examination of RESCAL for Completion of Knowledge Bases}},
year = {2014}
}
@article{Gardner2014,
abstract = {Abstract Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has ...},
author = {Gardner, Matt and Talukdar, Partha P. and Krishnamurthy, Jayant and Mitchell, Tom M.},
file = {:E$\backslash$:/论文集/Incorporating Vector Space Similarity in Random Walk Inference over.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {397--406},
title = {{Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases}},
year = {2014}
}
@article{Dong2014,
author = {Dong, Xin Luna and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Lao, Ni and Murphy, Kevin and Strohmann, Thomas and Sun, Shaohua and Zhang, Wei},
file = {:E$\backslash$:/论文集/Knowledge Vault A Web-Scale Approach to.pdf:pdf},
isbn = {9781450329569},
keywords = {els,information extraction,knowledge bases,machine learning,probabilistic mod-},
pages = {601--610},
title = {{Knowledge Vault : A Web-Scale Approach to Probabilistic Knowledge Fusion}},
year = {2014}
}
@article{Dong,
author = {Dong, Xin Luna and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Murphy, Kevin and Sun, Shaohua and Zhang, Wei},
file = {:E$\backslash$:/论文集/From Data Fusion to Knowledge Fusion.pdf:pdf},
pages = {881--892},
title = {{From Data Fusion to Knowledge Fusion}}
}
@article{West2014,
author = {West, Robert and Gabrilovich, Evgeniy and Murphy, Kevin and Sun, Shaohua and Gupta, Rahul and Lin, Dekang},
file = {:E$\backslash$:/论文集/Knowledge Base Completion via Search-Based.pdf:pdf},
isbn = {9781450327442},
title = {{Knowledge Base Completion via Search-Based Question Answering}},
year = {2014}
}
@article{Neelakantan2015,
abstract = {Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z) -{\textgreater} containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recursive neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11{\%}, and a method leveraging pre-trained embeddings by 7{\%}.},
archivePrefix = {arXiv},
arxivId = {1504.06662},
author = {Neelakantan, Arvind and Roth, Benjamin and McCallum, Andrew},
doi = {10.3115/v1/P15-1016},
eprint = {1504.06662},
file = {:E$\backslash$:/论文集/Compositional Vector Space Models for Knowledge Base Completion.pdf:pdf},
isbn = {9781941643723},
journal = {AAAI Spring Symposium Series},
keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
pages = {156--166},
title = {{Compositional Vector Space Models for Knowledge Base Completion}},
url = {http://aclweb.org/anthology/P15-1016},
year = {2015}
}
@article{Lao2012,
author = {Lao, Ni and Mitamura, Teruko and Mitchell, Tom and Technologies, Information},
file = {:E$\backslash$:/论文集/Efficient Random Walk Inference with knowledge graph.pdf:pdf},
journal = {PhD thesis},
title = {{Efficient Random Walk Inference with Knowledge Bases}},
year = {2012}
}
@article{Gardner2015,
abstract = {We explore some of the practicalities of using random walk inference methods, such as the Path Ranking Algorithm (PRA), for the task of knowledge base completion. We show that the random walk probabilities computed (at great expense) by PRA provide no discernible benefit to performance on this task, so they can safely be dropped. This allows us to define a simpler algorithm for generating feature matrices from graphs, which we call subgraph feature extraction (SFE). In addition to being conceptually simpler than PRA, SFE is much more efficient, reducing computation by an order of magnitude, and more expressive, allowing for much richer features than paths between two nodes in a graph. We show experimentally that this technique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.3044v2},
author = {Gardner, Matt and Mitchell, Tom},
eprint = {arXiv:1402.3044v2},
file = {:E$\backslash$:/论文集/Efficient and Expressive Knowledge Base Completion.pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of EMNLP},
number = {September},
pages = {1488--1498},
title = {{Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction}},
year = {2015}
}
@article{Bordes2014,
abstract = {Recent years have witnessed a proliferation of large-scale knowledge graphs, such as Freebase, YAGO, Google's Knowledge Graph, and Microsoft's Satori. Whereas there is a large body of research on mining homogeneous graphs, this new generation of information networks are highly heterogeneous, with thousands of entity and relation types and billions of instances of vertices and edges. In this tutorial, we will present the state of the art in constructing, mining, and growing knowledge graphs. The purpose of the tutorial is to equip newcomers to this exciting field with an understanding of the basic concepts, tools and methodologies, available datasets, and open research challenges. A publicly available knowledge base (Freebase) will be used throughout the tutorial to exemplify the different techniques.},
author = {Bordes, Antoine and Gabrilovich, Evgeniy},
doi = {10.1145/2623330.2630803},
file = {:E$\backslash$:/论文集/KDD14-T2-Bordes-Gabrilovich-KnowledgeGraph-Overview.pdf:pdf},
isbn = {9781450329569},
journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
pages = {1967--1967},
title = {{Constructing and mining web-scale knowledge graphs}},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2630803},
year = {2014}
}
@article{Nickel2016,
abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.},
archivePrefix = {arXiv},
arxivId = {1503.00759},
author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
doi = {10.1109/JPROC.2015.2483592},
eprint = {1503.00759},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickel et al. - 2016 - A review of relational machine learning for knowledge graphs.pdf:pdf},
isbn = {1089801300},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Graph-based models,knowledge extraction,knowledge graphs,latent feature models,statistical relational learning},
number = {1},
pages = {11--33},
title = {{A review of relational machine learning for knowledge graphs}},
volume = {104},
year = {2016}
}
@article{Galarraga2015,
author = {Gal{\'{a}}rraga, Luis and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian M.},
doi = {10.1007/s00778-015-0394-1},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal{\'{a}}rraga et al. - 2015 - Fast rule mining in ontological knowledge bases with AMIE.pdf:pdf},
isbn = {1066-8888},
issn = {0949877X},
journal = {VLDB Journal},
keywords = {ILP,Inductive logic programming,Knowledge bases,Rule mining},
number = {6},
pages = {707--730},
title = {{Fast rule mining in ontological knowledge bases with AMIE+}},
volume = {24},
year = {2015}
}
@article{Lao2015,
author = {Lao, N and Minkov, E and Cohen, Ww},
file = {:C$\backslash$:/Users/dell/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lao, Minkov, Cohen - 2015 - Learning Relational Features with Backward Random Walks.pdf:pdf},
isbn = {9781941643723},
journal = {Cs.Cmu.Edu},
keywords = {knowledge-base,logical inference,random walk},
pages = {666--675},
title = {{Learning Relational Features with Backward Random Walks}},
url = {http://www.cs.cmu.edu/{~}nlao/doc/ecml2012.pdf},
year = {2015}
}
